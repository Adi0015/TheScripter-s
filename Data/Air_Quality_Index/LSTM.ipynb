{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Load data\n",
    "df = pd.read_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 17:26:20.535853: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-02-20 17:26:20.536116: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 17:26:21.310268: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-02-20 17:26:21.905584: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-20 17:26:22.052986: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-20 17:26:23.704716: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-20 17:26:25.405078: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-20 17:26:25.445414: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 - 4s - loss: 0.1121 - val_loss: 0.0843 - 4s/epoch - 497ms/step\n",
      "Epoch 2/150\n",
      "9/9 - 0s - loss: 0.0621 - val_loss: 0.0457 - 141ms/epoch - 16ms/step\n",
      "Epoch 3/150\n",
      "9/9 - 0s - loss: 0.0367 - val_loss: 0.0385 - 163ms/epoch - 18ms/step\n",
      "Epoch 4/150\n",
      "9/9 - 0s - loss: 0.0345 - val_loss: 0.0353 - 169ms/epoch - 19ms/step\n",
      "Epoch 5/150\n",
      "9/9 - 0s - loss: 0.0329 - val_loss: 0.0343 - 156ms/epoch - 17ms/step\n",
      "Epoch 6/150\n",
      "9/9 - 0s - loss: 0.0318 - val_loss: 0.0340 - 149ms/epoch - 17ms/step\n",
      "Epoch 7/150\n",
      "9/9 - 0s - loss: 0.0313 - val_loss: 0.0336 - 170ms/epoch - 19ms/step\n",
      "Epoch 8/150\n",
      "9/9 - 0s - loss: 0.0309 - val_loss: 0.0331 - 150ms/epoch - 17ms/step\n",
      "Epoch 9/150\n",
      "9/9 - 0s - loss: 0.0305 - val_loss: 0.0327 - 158ms/epoch - 18ms/step\n",
      "Epoch 10/150\n",
      "9/9 - 0s - loss: 0.0302 - val_loss: 0.0324 - 171ms/epoch - 19ms/step\n",
      "Epoch 11/150\n",
      "9/9 - 0s - loss: 0.0298 - val_loss: 0.0321 - 147ms/epoch - 16ms/step\n",
      "Epoch 12/150\n",
      "9/9 - 0s - loss: 0.0295 - val_loss: 0.0318 - 163ms/epoch - 18ms/step\n",
      "Epoch 13/150\n",
      "9/9 - 0s - loss: 0.0292 - val_loss: 0.0314 - 156ms/epoch - 17ms/step\n",
      "Epoch 14/150\n",
      "9/9 - 0s - loss: 0.0289 - val_loss: 0.0311 - 163ms/epoch - 18ms/step\n",
      "Epoch 15/150\n",
      "9/9 - 0s - loss: 0.0286 - val_loss: 0.0308 - 148ms/epoch - 16ms/step\n",
      "Epoch 16/150\n",
      "9/9 - 0s - loss: 0.0284 - val_loss: 0.0305 - 162ms/epoch - 18ms/step\n",
      "Epoch 17/150\n",
      "9/9 - 0s - loss: 0.0281 - val_loss: 0.0303 - 150ms/epoch - 17ms/step\n",
      "Epoch 18/150\n",
      "9/9 - 0s - loss: 0.0279 - val_loss: 0.0300 - 146ms/epoch - 16ms/step\n",
      "Epoch 19/150\n",
      "9/9 - 0s - loss: 0.0276 - val_loss: 0.0297 - 152ms/epoch - 17ms/step\n",
      "Epoch 20/150\n",
      "9/9 - 0s - loss: 0.0274 - val_loss: 0.0295 - 156ms/epoch - 17ms/step\n",
      "Epoch 21/150\n",
      "9/9 - 0s - loss: 0.0272 - val_loss: 0.0293 - 135ms/epoch - 15ms/step\n",
      "Epoch 22/150\n",
      "9/9 - 0s - loss: 0.0270 - val_loss: 0.0290 - 174ms/epoch - 19ms/step\n",
      "Epoch 23/150\n",
      "9/9 - 0s - loss: 0.0268 - val_loss: 0.0288 - 140ms/epoch - 16ms/step\n",
      "Epoch 24/150\n",
      "9/9 - 0s - loss: 0.0267 - val_loss: 0.0286 - 147ms/epoch - 16ms/step\n",
      "Epoch 25/150\n",
      "9/9 - 0s - loss: 0.0265 - val_loss: 0.0284 - 152ms/epoch - 17ms/step\n",
      "Epoch 26/150\n",
      "9/9 - 0s - loss: 0.0263 - val_loss: 0.0283 - 138ms/epoch - 15ms/step\n",
      "Epoch 27/150\n",
      "9/9 - 0s - loss: 0.0262 - val_loss: 0.0281 - 161ms/epoch - 18ms/step\n",
      "Epoch 28/150\n",
      "9/9 - 0s - loss: 0.0261 - val_loss: 0.0279 - 149ms/epoch - 17ms/step\n",
      "Epoch 29/150\n",
      "9/9 - 0s - loss: 0.0260 - val_loss: 0.0278 - 160ms/epoch - 18ms/step\n",
      "Epoch 30/150\n",
      "9/9 - 0s - loss: 0.0258 - val_loss: 0.0276 - 150ms/epoch - 17ms/step\n",
      "Epoch 31/150\n",
      "9/9 - 0s - loss: 0.0257 - val_loss: 0.0275 - 144ms/epoch - 16ms/step\n",
      "Epoch 32/150\n",
      "9/9 - 0s - loss: 0.0256 - val_loss: 0.0274 - 164ms/epoch - 18ms/step\n",
      "Epoch 33/150\n",
      "9/9 - 0s - loss: 0.0255 - val_loss: 0.0273 - 167ms/epoch - 19ms/step\n",
      "Epoch 34/150\n",
      "9/9 - 0s - loss: 0.0254 - val_loss: 0.0271 - 160ms/epoch - 18ms/step\n",
      "Epoch 35/150\n",
      "9/9 - 0s - loss: 0.0253 - val_loss: 0.0270 - 144ms/epoch - 16ms/step\n",
      "Epoch 36/150\n",
      "9/9 - 0s - loss: 0.0252 - val_loss: 0.0269 - 143ms/epoch - 16ms/step\n",
      "Epoch 37/150\n",
      "9/9 - 0s - loss: 0.0252 - val_loss: 0.0269 - 165ms/epoch - 18ms/step\n",
      "Epoch 38/150\n",
      "9/9 - 0s - loss: 0.0251 - val_loss: 0.0268 - 161ms/epoch - 18ms/step\n",
      "Epoch 39/150\n",
      "9/9 - 0s - loss: 0.0250 - val_loss: 0.0267 - 147ms/epoch - 16ms/step\n",
      "Epoch 40/150\n",
      "9/9 - 0s - loss: 0.0249 - val_loss: 0.0266 - 165ms/epoch - 18ms/step\n",
      "Epoch 41/150\n",
      "9/9 - 0s - loss: 0.0249 - val_loss: 0.0265 - 140ms/epoch - 16ms/step\n",
      "Epoch 42/150\n",
      "9/9 - 0s - loss: 0.0248 - val_loss: 0.0265 - 161ms/epoch - 18ms/step\n",
      "Epoch 43/150\n",
      "9/9 - 0s - loss: 0.0247 - val_loss: 0.0264 - 158ms/epoch - 18ms/step\n",
      "Epoch 44/150\n",
      "9/9 - 0s - loss: 0.0246 - val_loss: 0.0263 - 144ms/epoch - 16ms/step\n",
      "Epoch 45/150\n",
      "9/9 - 0s - loss: 0.0246 - val_loss: 0.0263 - 151ms/epoch - 17ms/step\n",
      "Epoch 46/150\n",
      "9/9 - 0s - loss: 0.0245 - val_loss: 0.0262 - 161ms/epoch - 18ms/step\n",
      "Epoch 47/150\n",
      "9/9 - 0s - loss: 0.0244 - val_loss: 0.0262 - 137ms/epoch - 15ms/step\n",
      "Epoch 48/150\n",
      "9/9 - 0s - loss: 0.0243 - val_loss: 0.0261 - 149ms/epoch - 17ms/step\n",
      "Epoch 49/150\n",
      "9/9 - 0s - loss: 0.0243 - val_loss: 0.0260 - 164ms/epoch - 18ms/step\n",
      "Epoch 50/150\n",
      "9/9 - 0s - loss: 0.0242 - val_loss: 0.0260 - 160ms/epoch - 18ms/step\n",
      "Epoch 51/150\n",
      "9/9 - 0s - loss: 0.0241 - val_loss: 0.0259 - 150ms/epoch - 17ms/step\n",
      "Epoch 52/150\n",
      "9/9 - 0s - loss: 0.0240 - val_loss: 0.0259 - 160ms/epoch - 18ms/step\n",
      "Epoch 53/150\n",
      "9/9 - 0s - loss: 0.0239 - val_loss: 0.0258 - 159ms/epoch - 18ms/step\n",
      "Epoch 54/150\n",
      "9/9 - 0s - loss: 0.0239 - val_loss: 0.0257 - 159ms/epoch - 18ms/step\n",
      "Epoch 55/150\n",
      "9/9 - 0s - loss: 0.0238 - val_loss: 0.0257 - 166ms/epoch - 18ms/step\n",
      "Epoch 56/150\n",
      "9/9 - 0s - loss: 0.0237 - val_loss: 0.0256 - 159ms/epoch - 18ms/step\n",
      "Epoch 57/150\n",
      "9/9 - 0s - loss: 0.0236 - val_loss: 0.0255 - 154ms/epoch - 17ms/step\n",
      "Epoch 58/150\n",
      "9/9 - 0s - loss: 0.0235 - val_loss: 0.0254 - 144ms/epoch - 16ms/step\n",
      "Epoch 59/150\n",
      "9/9 - 0s - loss: 0.0234 - val_loss: 0.0254 - 155ms/epoch - 17ms/step\n",
      "Epoch 60/150\n",
      "9/9 - 0s - loss: 0.0233 - val_loss: 0.0253 - 133ms/epoch - 15ms/step\n",
      "Epoch 61/150\n",
      "9/9 - 0s - loss: 0.0232 - val_loss: 0.0252 - 151ms/epoch - 17ms/step\n",
      "Epoch 62/150\n",
      "9/9 - 0s - loss: 0.0231 - val_loss: 0.0251 - 165ms/epoch - 18ms/step\n",
      "Epoch 63/150\n",
      "9/9 - 0s - loss: 0.0230 - val_loss: 0.0250 - 161ms/epoch - 18ms/step\n",
      "Epoch 64/150\n",
      "9/9 - 0s - loss: 0.0229 - val_loss: 0.0248 - 162ms/epoch - 18ms/step\n",
      "Epoch 65/150\n",
      "9/9 - 0s - loss: 0.0228 - val_loss: 0.0247 - 167ms/epoch - 19ms/step\n",
      "Epoch 66/150\n",
      "9/9 - 0s - loss: 0.0227 - val_loss: 0.0245 - 153ms/epoch - 17ms/step\n",
      "Epoch 67/150\n",
      "9/9 - 0s - loss: 0.0226 - val_loss: 0.0244 - 154ms/epoch - 17ms/step\n",
      "Epoch 68/150\n",
      "9/9 - 0s - loss: 0.0225 - val_loss: 0.0242 - 150ms/epoch - 17ms/step\n",
      "Epoch 69/150\n",
      "9/9 - 0s - loss: 0.0224 - val_loss: 0.0241 - 159ms/epoch - 18ms/step\n",
      "Epoch 70/150\n",
      "9/9 - 0s - loss: 0.0223 - val_loss: 0.0240 - 161ms/epoch - 18ms/step\n",
      "Epoch 71/150\n",
      "9/9 - 0s - loss: 0.0222 - val_loss: 0.0238 - 147ms/epoch - 16ms/step\n",
      "Epoch 72/150\n",
      "9/9 - 0s - loss: 0.0220 - val_loss: 0.0237 - 158ms/epoch - 18ms/step\n",
      "Epoch 73/150\n",
      "9/9 - 0s - loss: 0.0219 - val_loss: 0.0236 - 149ms/epoch - 17ms/step\n",
      "Epoch 74/150\n",
      "9/9 - 0s - loss: 0.0217 - val_loss: 0.0235 - 152ms/epoch - 17ms/step\n",
      "Epoch 75/150\n",
      "9/9 - 0s - loss: 0.0216 - val_loss: 0.0233 - 158ms/epoch - 18ms/step\n",
      "Epoch 76/150\n",
      "9/9 - 0s - loss: 0.0215 - val_loss: 0.0232 - 152ms/epoch - 17ms/step\n",
      "Epoch 77/150\n",
      "9/9 - 0s - loss: 0.0213 - val_loss: 0.0232 - 152ms/epoch - 17ms/step\n",
      "Epoch 78/150\n",
      "9/9 - 0s - loss: 0.0212 - val_loss: 0.0231 - 162ms/epoch - 18ms/step\n",
      "Epoch 79/150\n",
      "9/9 - 0s - loss: 0.0211 - val_loss: 0.0231 - 163ms/epoch - 18ms/step\n",
      "Epoch 80/150\n",
      "9/9 - 0s - loss: 0.0209 - val_loss: 0.0231 - 155ms/epoch - 17ms/step\n",
      "Epoch 81/150\n",
      "9/9 - 0s - loss: 0.0208 - val_loss: 0.0231 - 153ms/epoch - 17ms/step\n",
      "Epoch 82/150\n",
      "9/9 - 0s - loss: 0.0207 - val_loss: 0.0230 - 155ms/epoch - 17ms/step\n",
      "Epoch 83/150\n",
      "9/9 - 0s - loss: 0.0206 - val_loss: 0.0229 - 137ms/epoch - 15ms/step\n",
      "Epoch 84/150\n",
      "9/9 - 0s - loss: 0.0205 - val_loss: 0.0227 - 147ms/epoch - 16ms/step\n",
      "Epoch 85/150\n",
      "9/9 - 0s - loss: 0.0203 - val_loss: 0.0226 - 148ms/epoch - 16ms/step\n",
      "Epoch 86/150\n",
      "9/9 - 0s - loss: 0.0202 - val_loss: 0.0226 - 160ms/epoch - 18ms/step\n",
      "Epoch 87/150\n",
      "9/9 - 0s - loss: 0.0201 - val_loss: 0.0225 - 168ms/epoch - 19ms/step\n",
      "Epoch 88/150\n",
      "9/9 - 0s - loss: 0.0200 - val_loss: 0.0225 - 162ms/epoch - 18ms/step\n",
      "Epoch 89/150\n",
      "9/9 - 0s - loss: 0.0199 - val_loss: 0.0225 - 159ms/epoch - 18ms/step\n",
      "Epoch 90/150\n",
      "9/9 - 0s - loss: 0.0198 - val_loss: 0.0225 - 155ms/epoch - 17ms/step\n",
      "Epoch 91/150\n",
      "9/9 - 0s - loss: 0.0197 - val_loss: 0.0225 - 141ms/epoch - 16ms/step\n",
      "Epoch 92/150\n",
      "9/9 - 0s - loss: 0.0196 - val_loss: 0.0225 - 151ms/epoch - 17ms/step\n",
      "Epoch 93/150\n",
      "9/9 - 0s - loss: 0.0195 - val_loss: 0.0225 - 158ms/epoch - 18ms/step\n",
      "Epoch 94/150\n",
      "9/9 - 0s - loss: 0.0194 - val_loss: 0.0225 - 141ms/epoch - 16ms/step\n",
      "Epoch 95/150\n",
      "9/9 - 0s - loss: 0.0193 - val_loss: 0.0225 - 154ms/epoch - 17ms/step\n",
      "Epoch 96/150\n",
      "9/9 - 0s - loss: 0.0192 - val_loss: 0.0225 - 163ms/epoch - 18ms/step\n",
      "Epoch 97/150\n",
      "9/9 - 0s - loss: 0.0192 - val_loss: 0.0225 - 159ms/epoch - 18ms/step\n",
      "Epoch 98/150\n",
      "9/9 - 0s - loss: 0.0191 - val_loss: 0.0225 - 152ms/epoch - 17ms/step\n",
      "Epoch 99/150\n",
      "9/9 - 0s - loss: 0.0190 - val_loss: 0.0226 - 147ms/epoch - 16ms/step\n",
      "Epoch 100/150\n",
      "9/9 - 0s - loss: 0.0189 - val_loss: 0.0227 - 159ms/epoch - 18ms/step\n",
      "Epoch 101/150\n",
      "9/9 - 0s - loss: 0.0189 - val_loss: 0.0228 - 159ms/epoch - 18ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/150\n",
      "9/9 - 0s - loss: 0.0188 - val_loss: 0.0229 - 159ms/epoch - 18ms/step\n",
      "Epoch 103/150\n",
      "9/9 - 0s - loss: 0.0188 - val_loss: 0.0230 - 159ms/epoch - 18ms/step\n",
      "Epoch 104/150\n",
      "9/9 - 0s - loss: 0.0188 - val_loss: 0.0231 - 160ms/epoch - 18ms/step\n",
      "Epoch 105/150\n",
      "9/9 - 0s - loss: 0.0188 - val_loss: 0.0230 - 151ms/epoch - 17ms/step\n",
      "Epoch 106/150\n",
      "9/9 - 0s - loss: 0.0189 - val_loss: 0.0225 - 152ms/epoch - 17ms/step\n",
      "Epoch 107/150\n",
      "9/9 - 0s - loss: 0.0188 - val_loss: 0.0223 - 143ms/epoch - 16ms/step\n",
      "Epoch 108/150\n",
      "9/9 - 0s - loss: 0.0186 - val_loss: 0.0223 - 155ms/epoch - 17ms/step\n",
      "Epoch 109/150\n",
      "9/9 - 0s - loss: 0.0185 - val_loss: 0.0224 - 160ms/epoch - 18ms/step\n",
      "Epoch 110/150\n",
      "9/9 - 0s - loss: 0.0184 - val_loss: 0.0224 - 161ms/epoch - 18ms/step\n",
      "Epoch 111/150\n",
      "9/9 - 0s - loss: 0.0183 - val_loss: 0.0225 - 148ms/epoch - 16ms/step\n",
      "Epoch 112/150\n",
      "9/9 - 0s - loss: 0.0182 - val_loss: 0.0225 - 152ms/epoch - 17ms/step\n",
      "Epoch 113/150\n",
      "9/9 - 0s - loss: 0.0182 - val_loss: 0.0226 - 166ms/epoch - 18ms/step\n",
      "Epoch 114/150\n",
      "9/9 - 0s - loss: 0.0181 - val_loss: 0.0226 - 157ms/epoch - 17ms/step\n",
      "Epoch 115/150\n",
      "9/9 - 0s - loss: 0.0181 - val_loss: 0.0225 - 165ms/epoch - 18ms/step\n",
      "Epoch 116/150\n",
      "9/9 - 0s - loss: 0.0181 - val_loss: 0.0225 - 158ms/epoch - 18ms/step\n",
      "Epoch 117/150\n",
      "9/9 - 0s - loss: 0.0180 - val_loss: 0.0224 - 165ms/epoch - 18ms/step\n",
      "Epoch 118/150\n",
      "9/9 - 0s - loss: 0.0180 - val_loss: 0.0224 - 165ms/epoch - 18ms/step\n",
      "Epoch 119/150\n",
      "9/9 - 0s - loss: 0.0179 - val_loss: 0.0224 - 148ms/epoch - 16ms/step\n",
      "Epoch 120/150\n",
      "9/9 - 0s - loss: 0.0179 - val_loss: 0.0224 - 146ms/epoch - 16ms/step\n",
      "Epoch 121/150\n",
      "9/9 - 0s - loss: 0.0178 - val_loss: 0.0225 - 141ms/epoch - 16ms/step\n",
      "Epoch 122/150\n",
      "9/9 - 0s - loss: 0.0178 - val_loss: 0.0225 - 159ms/epoch - 18ms/step\n",
      "Epoch 123/150\n",
      "9/9 - 0s - loss: 0.0177 - val_loss: 0.0224 - 172ms/epoch - 19ms/step\n",
      "Epoch 124/150\n",
      "9/9 - 0s - loss: 0.0177 - val_loss: 0.0224 - 156ms/epoch - 17ms/step\n",
      "Epoch 125/150\n",
      "9/9 - 0s - loss: 0.0176 - val_loss: 0.0224 - 153ms/epoch - 17ms/step\n",
      "Epoch 126/150\n",
      "9/9 - 0s - loss: 0.0176 - val_loss: 0.0224 - 164ms/epoch - 18ms/step\n",
      "Epoch 127/150\n",
      "9/9 - 0s - loss: 0.0175 - val_loss: 0.0224 - 175ms/epoch - 19ms/step\n",
      "Epoch 128/150\n",
      "9/9 - 0s - loss: 0.0175 - val_loss: 0.0224 - 136ms/epoch - 15ms/step\n",
      "Epoch 129/150\n",
      "9/9 - 0s - loss: 0.0175 - val_loss: 0.0223 - 171ms/epoch - 19ms/step\n",
      "Epoch 130/150\n",
      "9/9 - 0s - loss: 0.0174 - val_loss: 0.0223 - 161ms/epoch - 18ms/step\n",
      "Epoch 131/150\n",
      "9/9 - 0s - loss: 0.0174 - val_loss: 0.0223 - 158ms/epoch - 18ms/step\n",
      "Epoch 132/150\n",
      "9/9 - 0s - loss: 0.0173 - val_loss: 0.0223 - 159ms/epoch - 18ms/step\n",
      "Epoch 133/150\n",
      "9/9 - 0s - loss: 0.0173 - val_loss: 0.0223 - 159ms/epoch - 18ms/step\n",
      "Epoch 134/150\n",
      "9/9 - 0s - loss: 0.0173 - val_loss: 0.0223 - 159ms/epoch - 18ms/step\n",
      "Epoch 135/150\n",
      "9/9 - 0s - loss: 0.0173 - val_loss: 0.0223 - 165ms/epoch - 18ms/step\n",
      "Epoch 136/150\n",
      "9/9 - 0s - loss: 0.0172 - val_loss: 0.0222 - 165ms/epoch - 18ms/step\n",
      "Epoch 137/150\n",
      "9/9 - 0s - loss: 0.0172 - val_loss: 0.0222 - 164ms/epoch - 18ms/step\n",
      "Epoch 138/150\n",
      "9/9 - 0s - loss: 0.0171 - val_loss: 0.0222 - 161ms/epoch - 18ms/step\n",
      "Epoch 139/150\n",
      "9/9 - 0s - loss: 0.0171 - val_loss: 0.0222 - 158ms/epoch - 18ms/step\n",
      "Epoch 140/150\n",
      "9/9 - 0s - loss: 0.0170 - val_loss: 0.0221 - 139ms/epoch - 15ms/step\n",
      "Epoch 141/150\n",
      "9/9 - 0s - loss: 0.0170 - val_loss: 0.0221 - 160ms/epoch - 18ms/step\n",
      "Epoch 142/150\n",
      "9/9 - 0s - loss: 0.0169 - val_loss: 0.0221 - 134ms/epoch - 15ms/step\n",
      "Epoch 143/150\n",
      "9/9 - 0s - loss: 0.0169 - val_loss: 0.0221 - 164ms/epoch - 18ms/step\n",
      "Epoch 144/150\n",
      "9/9 - 0s - loss: 0.0169 - val_loss: 0.0221 - 157ms/epoch - 17ms/step\n",
      "Epoch 145/150\n",
      "9/9 - 0s - loss: 0.0168 - val_loss: 0.0220 - 156ms/epoch - 17ms/step\n",
      "Epoch 146/150\n",
      "9/9 - 0s - loss: 0.0168 - val_loss: 0.0220 - 139ms/epoch - 15ms/step\n",
      "Epoch 147/150\n",
      "9/9 - 0s - loss: 0.0168 - val_loss: 0.0220 - 161ms/epoch - 18ms/step\n",
      "Epoch 148/150\n",
      "9/9 - 0s - loss: 0.0168 - val_loss: 0.0220 - 166ms/epoch - 18ms/step\n",
      "Epoch 149/150\n",
      "9/9 - 0s - loss: 0.0168 - val_loss: 0.0219 - 157ms/epoch - 17ms/step\n",
      "Epoch 150/150\n",
      "9/9 - 0s - loss: 0.0167 - val_loss: 0.0220 - 158ms/epoch - 18ms/step\n",
      "Test RMSE: 71.225\n",
      "[146.2151     7.901723   9.015945  64.10015   58.61056 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 17:26:49.261758: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-20 17:26:49.296308: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter data for a given location\n",
    "location_id =0\n",
    "df = df[df[\"Location_ID\"] == location_id]\n",
    "\n",
    "# Convert date column to datetime format\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "# Sort data by date\n",
    "df.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "# Extract target variables\n",
    "targets = [\"AQI\", \"NO2\", \"SO2\", \"PM10\", \"PM2.5\"]\n",
    "y = df[targets]\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled = scaler.fit_transform(y)\n",
    "\n",
    "# Define sequence length for LSTM\n",
    "seq_length = 30\n",
    "\n",
    "# Create input and output sequences\n",
    "X, Y = [], []\n",
    "for i in range(len(y_scaled) - seq_length):\n",
    "    X.append(y_scaled[i:i+seq_length])\n",
    "    Y.append(y_scaled[i+seq_length])\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Define model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(32, input_shape=(seq_length, len(targets))))\n",
    "model.add(Dense(len(targets)))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "# Train model\n",
    "history=model.fit(X_train, y_train, batch_size=64, epochs=150,validation_data=(X_test,y_test),verbose=2,shuffle=False)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "y_pred = scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# Calculate root mean squared error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "print(y_pred[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(model, X_test, scaler, seq_length, targets, start_date, end_date, freq='D'):\n",
    "    \"\"\"\n",
    "    Generates predictions for future dates using a trained LSTM model.\n",
    "\n",
    "    Args:\n",
    "    - model: a trained Keras LSTM model\n",
    "    - X_test: a numpy array of shape (num_samples, seq_length, num_features) containing test data\n",
    "    - scaler: a fitted sklearn.preprocessing.MinMaxScaler object used to scale the data\n",
    "    - seq_length: the sequence length used for training the model\n",
    "    - targets: a list of target column names\n",
    "    - start_date: the start date for the future date range (string in 'YYYY-MM-DD' format)\n",
    "    - end_date: the end date for the future date range (string in 'YYYY-MM-DD' format)\n",
    "    - freq: the frequency of the future date range (default='D')\n",
    "\n",
    "    Returns:\n",
    "    - preds_df: a pandas DataFrame of shape (num_predictions, num_targets) containing the predicted values\n",
    "    \"\"\"\n",
    "    # Define future date range\n",
    "    future_dates = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "\n",
    "    # Create input sequences for future dates\n",
    "    last_seq = np.array(X_test[-1])  # last sequence from the test data\n",
    "    input_seq = last_seq.copy()\n",
    "    preds = []\n",
    "    for i in range(len(future_dates)):\n",
    "        pred = model.predict(input_seq.reshape(1, seq_length, len(targets)))\n",
    "        preds.append(pred)\n",
    "        input_seq = np.append(input_seq[1:], pred, axis=0)\n",
    "\n",
    "    # Inverse transform the predicted values\n",
    "    preds = np.array(preds).reshape(-1, len(targets))\n",
    "    preds_inv = scaler.inverse_transform(preds)\n",
    "\n",
    "    # Convert predictions and dates to a pandas DataFrame\n",
    "    preds_df = pd.DataFrame(preds_inv, index=future_dates, columns=targets)\n",
    "\n",
    "    return preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AQI</th>\n",
       "      <th>NO2</th>\n",
       "      <th>SO2</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM2.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-02-01</th>\n",
       "      <td>176.575867</td>\n",
       "      <td>9.789560</td>\n",
       "      <td>12.242630</td>\n",
       "      <td>87.781151</td>\n",
       "      <td>70.188988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-02</th>\n",
       "      <td>163.973999</td>\n",
       "      <td>9.808427</td>\n",
       "      <td>11.526342</td>\n",
       "      <td>80.622795</td>\n",
       "      <td>61.683750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-03</th>\n",
       "      <td>160.355194</td>\n",
       "      <td>9.602151</td>\n",
       "      <td>10.914307</td>\n",
       "      <td>78.587379</td>\n",
       "      <td>56.834553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-04</th>\n",
       "      <td>165.058640</td>\n",
       "      <td>9.539268</td>\n",
       "      <td>10.583435</td>\n",
       "      <td>81.026688</td>\n",
       "      <td>55.284950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-05</th>\n",
       "      <td>172.777603</td>\n",
       "      <td>9.566977</td>\n",
       "      <td>10.372444</td>\n",
       "      <td>84.743294</td>\n",
       "      <td>56.627804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-06</th>\n",
       "      <td>176.755966</td>\n",
       "      <td>9.474874</td>\n",
       "      <td>10.283803</td>\n",
       "      <td>87.153915</td>\n",
       "      <td>60.499985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-07</th>\n",
       "      <td>175.277313</td>\n",
       "      <td>9.086517</td>\n",
       "      <td>10.616331</td>\n",
       "      <td>88.633362</td>\n",
       "      <td>65.583176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-08</th>\n",
       "      <td>169.282806</td>\n",
       "      <td>8.553928</td>\n",
       "      <td>10.499880</td>\n",
       "      <td>84.435562</td>\n",
       "      <td>65.015038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-09</th>\n",
       "      <td>162.239243</td>\n",
       "      <td>7.979759</td>\n",
       "      <td>10.192388</td>\n",
       "      <td>79.235786</td>\n",
       "      <td>60.864849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-10</th>\n",
       "      <td>154.592194</td>\n",
       "      <td>7.532444</td>\n",
       "      <td>9.784504</td>\n",
       "      <td>74.387993</td>\n",
       "      <td>55.419624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-11</th>\n",
       "      <td>142.970215</td>\n",
       "      <td>7.173999</td>\n",
       "      <td>9.068937</td>\n",
       "      <td>65.025490</td>\n",
       "      <td>46.623867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-12</th>\n",
       "      <td>132.579117</td>\n",
       "      <td>7.179476</td>\n",
       "      <td>8.277090</td>\n",
       "      <td>56.203056</td>\n",
       "      <td>38.472748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-13</th>\n",
       "      <td>130.915756</td>\n",
       "      <td>7.420330</td>\n",
       "      <td>7.875399</td>\n",
       "      <td>54.209663</td>\n",
       "      <td>35.068016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-14</th>\n",
       "      <td>143.592911</td>\n",
       "      <td>7.747770</td>\n",
       "      <td>8.256156</td>\n",
       "      <td>62.628082</td>\n",
       "      <td>38.931595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-15</th>\n",
       "      <td>165.374512</td>\n",
       "      <td>7.976944</td>\n",
       "      <td>9.413333</td>\n",
       "      <td>80.672272</td>\n",
       "      <td>51.754517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-16</th>\n",
       "      <td>181.254669</td>\n",
       "      <td>8.505679</td>\n",
       "      <td>10.568246</td>\n",
       "      <td>91.471687</td>\n",
       "      <td>63.498886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-17</th>\n",
       "      <td>177.265961</td>\n",
       "      <td>8.648615</td>\n",
       "      <td>10.355254</td>\n",
       "      <td>82.415932</td>\n",
       "      <td>62.419838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-18</th>\n",
       "      <td>168.369110</td>\n",
       "      <td>8.077685</td>\n",
       "      <td>9.612395</td>\n",
       "      <td>70.804718</td>\n",
       "      <td>55.543537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-19</th>\n",
       "      <td>163.419617</td>\n",
       "      <td>7.460787</td>\n",
       "      <td>9.022439</td>\n",
       "      <td>63.673615</td>\n",
       "      <td>47.771130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-20</th>\n",
       "      <td>162.170746</td>\n",
       "      <td>7.331342</td>\n",
       "      <td>8.681009</td>\n",
       "      <td>60.348267</td>\n",
       "      <td>40.832012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-21</th>\n",
       "      <td>163.070206</td>\n",
       "      <td>7.597989</td>\n",
       "      <td>8.343946</td>\n",
       "      <td>60.339409</td>\n",
       "      <td>36.278034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-22</th>\n",
       "      <td>166.378296</td>\n",
       "      <td>7.811953</td>\n",
       "      <td>8.042706</td>\n",
       "      <td>64.428787</td>\n",
       "      <td>36.504772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-23</th>\n",
       "      <td>173.595352</td>\n",
       "      <td>7.945037</td>\n",
       "      <td>8.112164</td>\n",
       "      <td>72.731804</td>\n",
       "      <td>41.710197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-24</th>\n",
       "      <td>181.038040</td>\n",
       "      <td>8.057907</td>\n",
       "      <td>8.578364</td>\n",
       "      <td>81.538551</td>\n",
       "      <td>50.421612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-25</th>\n",
       "      <td>183.502777</td>\n",
       "      <td>8.291501</td>\n",
       "      <td>9.027938</td>\n",
       "      <td>83.926338</td>\n",
       "      <td>57.288563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-26</th>\n",
       "      <td>178.439606</td>\n",
       "      <td>8.210683</td>\n",
       "      <td>9.269302</td>\n",
       "      <td>78.556213</td>\n",
       "      <td>59.504890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-27</th>\n",
       "      <td>169.156418</td>\n",
       "      <td>7.741117</td>\n",
       "      <td>9.016727</td>\n",
       "      <td>68.691841</td>\n",
       "      <td>54.547440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-28</th>\n",
       "      <td>161.778015</td>\n",
       "      <td>7.140932</td>\n",
       "      <td>8.657269</td>\n",
       "      <td>60.601616</td>\n",
       "      <td>45.870037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   AQI       NO2        SO2       PM10      PM2.5\n",
       "2023-02-01  176.575867  9.789560  12.242630  87.781151  70.188988\n",
       "2023-02-02  163.973999  9.808427  11.526342  80.622795  61.683750\n",
       "2023-02-03  160.355194  9.602151  10.914307  78.587379  56.834553\n",
       "2023-02-04  165.058640  9.539268  10.583435  81.026688  55.284950\n",
       "2023-02-05  172.777603  9.566977  10.372444  84.743294  56.627804\n",
       "2023-02-06  176.755966  9.474874  10.283803  87.153915  60.499985\n",
       "2023-02-07  175.277313  9.086517  10.616331  88.633362  65.583176\n",
       "2023-02-08  169.282806  8.553928  10.499880  84.435562  65.015038\n",
       "2023-02-09  162.239243  7.979759  10.192388  79.235786  60.864849\n",
       "2023-02-10  154.592194  7.532444   9.784504  74.387993  55.419624\n",
       "2023-02-11  142.970215  7.173999   9.068937  65.025490  46.623867\n",
       "2023-02-12  132.579117  7.179476   8.277090  56.203056  38.472748\n",
       "2023-02-13  130.915756  7.420330   7.875399  54.209663  35.068016\n",
       "2023-02-14  143.592911  7.747770   8.256156  62.628082  38.931595\n",
       "2023-02-15  165.374512  7.976944   9.413333  80.672272  51.754517\n",
       "2023-02-16  181.254669  8.505679  10.568246  91.471687  63.498886\n",
       "2023-02-17  177.265961  8.648615  10.355254  82.415932  62.419838\n",
       "2023-02-18  168.369110  8.077685   9.612395  70.804718  55.543537\n",
       "2023-02-19  163.419617  7.460787   9.022439  63.673615  47.771130\n",
       "2023-02-20  162.170746  7.331342   8.681009  60.348267  40.832012\n",
       "2023-02-21  163.070206  7.597989   8.343946  60.339409  36.278034\n",
       "2023-02-22  166.378296  7.811953   8.042706  64.428787  36.504772\n",
       "2023-02-23  173.595352  7.945037   8.112164  72.731804  41.710197\n",
       "2023-02-24  181.038040  8.057907   8.578364  81.538551  50.421612\n",
       "2023-02-25  183.502777  8.291501   9.027938  83.926338  57.288563\n",
       "2023-02-26  178.439606  8.210683   9.269302  78.556213  59.504890\n",
       "2023-02-27  169.156418  7.741117   9.016727  68.691841  54.547440\n",
       "2023-02-28  161.778015  7.140932   8.657269  60.601616  45.870037"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_future(model, X_test, scaler, seq_length, targets, start_date='2023-02-01', end_date='2023-02-28', freq='D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
